# Architecture Jarvis - Documentation Technique

> Documentation d√©taill√©e de l'architecture actuelle du projet Jarvis

**Date**: 2025-12-16
**Version**: 0.1.0
**√âtat**: Phase 3 compl√©t√©e (Voice Pipeline op√©rationnel)

---

## Table des Mati√®res

- [Vue d'Ensemble](#vue-densemble)
- [Architecture des Modules](#architecture-des-modules)
- [Pipeline de Traitement](#pipeline-de-traitement)
- [Flux de Donn√©es](#flux-de-donn√©es)
- [Impl√©mentation Actuelle](#impl√©mentation-actuelle)
- [Modules √Ä Impl√©menter](#modules-√†-impl√©menter)
- [D√©cisions Architecturales](#d√©cisions-architecturales)

---

## Vue d'Ensemble

### Principes d'Architecture

1. **Modularit√©**: Chaque composant (STT, TTS, Agent, Graph) est ind√©pendant
2. **Async-first**: Toutes les op√©rations I/O utilisent async/await
3. **Singleton Pattern**: Providers charg√©s une seule fois (lazy loading)
4. **Configuration centralis√©e**: Environnement via `.env`
5. **Logging unifi√©**: Loguru pour tous les modules
6. **API-first**: FastAPI expose tous les services
7. **Containerisation**: Docker pour isolation et d√©ploiement

### Stack Technologique Actuelle

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Frontend (Browser)              ‚îÇ
‚îÇ  HTML5 + CSS3 + Vanilla JavaScript      ‚îÇ
‚îÇ  MediaRecorder + Canvas + Web Audio     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ HTTP/WebM
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Backend (Docker Container)       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  FastAPI (Python 3.11)            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Uvicorn ASGI Server            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - CORS Middleware                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Static Files Serving           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                  ‚îÇ                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ               ‚îÇ                   ‚îÇ  ‚îÇ
‚îÇ  ‚ñº               ‚ñº                   ‚ñº  ‚îÇ
‚îÇ  STT           Agent                TTS ‚îÇ
‚îÇ  (Whisper)     (Claude)        (EdgeTTS)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ Neo4j Bolt Protocol
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Neo4j (Docker Container)            ‚îÇ
‚îÇ  - Graph Database 5.15                  ‚îÇ
‚îÇ  - APOC Plugin                          ‚îÇ
‚îÇ  - Graphiti Framework                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Architecture des Modules

### 1. API Layer (`src/api/main.py`)

**Responsabilit√©**: Exposition HTTP des services

```python
FastAPI Application
‚îú‚îÄ‚îÄ Endpoints
‚îÇ   ‚îú‚îÄ‚îÄ GET  /               # Serve web interface
‚îÇ   ‚îú‚îÄ‚îÄ GET  /health         # Health check
‚îÇ   ‚îú‚îÄ‚îÄ POST /api/voice/process  # Main voice pipeline
‚îÇ   ‚îú‚îÄ‚îÄ GET  /api/knowledge/query  # Knowledge graph query (TODO)
‚îÇ   ‚îî‚îÄ‚îÄ POST /api/knowledge/add    # Add knowledge (TODO)
‚îú‚îÄ‚îÄ Middleware
‚îÇ   ‚îî‚îÄ‚îÄ CORS (allow all origins for ESP32)
‚îî‚îÄ‚îÄ Static Files
    ‚îî‚îÄ‚îÄ /static/* (index.html, app.js, response_*.mp3)
```

**Impl√©mentation**:
- 188 lignes de code
- Async request handlers
- Gestion fichiers temporaires (`/app/data/temp/`)
- UUID pour identifiants uniques de requ√™tes
- Error handling avec HTTPException

**D√©pendances**:
```python
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from src.voice.stt import transcribe_audio
from src.voice.tts import synthesize_speech
from src.agents.jarvis_agent import get_agent
```

---

### 2. Voice Processing Layer

#### 2.1 Speech-to-Text (`src/voice/stt.py`)

**Responsabilit√©**: Conversion audio ‚Üí texte

**Providers Impl√©ment√©s**:

| Provider | Type | Avantages | Inconv√©nients |
|----------|------|-----------|---------------|
| **WhisperLocalSTT** | Local | Gratuit, privacy, multilangue | Lent (CPU), 1√®re utilisation t√©l√©charge mod√®le |
| **GroqSTT** | Cloud | Rapide, gratuit | Requiert cl√© API, pas de privacy |

**Architecture**:
```python
class STTProvider(ABC):
    @abstractmethod
    async def transcribe(file_path, language) -> str
        # Interface commune

class WhisperLocalSTT(STTProvider):
    _model = None  # Singleton, lazy loading

    def _load_model(self):
        # Charge en m√©moire une seule fois
        # Mod√®les: tiny, base, small, medium, large

    async def transcribe(self, file_path, language):
        # Conversion WebM ‚Üí WAV ‚Üí Transcription
        # ffmpeg requis pour conversion format

class GroqSTT(STTProvider):
    async def transcribe(self, file_path, language):
        # API call to Groq Whisper endpoint

# Factory function
async def transcribe_audio(file_path, language="fr") -> str:
    provider = get_stt_provider()  # Based on .env
    return await provider.transcribe(file_path, language)
```

**Configuration (.env)**:
```bash
STT_PROVIDER=whisper-local  # ou "groq"
STT_MODEL=base              # tiny, base, small, medium, large
GROQ_API_KEY=gsk_xxx        # si provider=groq
```

**Formats support√©s**: WebM, WAV, MP3, M4A (via ffmpeg)

---

#### 2.2 Text-to-Speech (`src/voice/tts.py`)

**Responsabilit√©**: Conversion texte ‚Üí audio

**Providers Impl√©ment√©s**:

| Provider | Type | Avantages | Inconv√©nients |
|----------|------|-----------|---------------|
| **EdgeTTSProvider** | Cloud | Gratuit, haute qualit√©, voix naturelles | Requiert connexion internet |
| **CoquiTTSProvider** | Local | Privacy, offline | Qualit√© inf√©rieure, lent |

**Architecture**:
```python
class TTSProvider(ABC):
    @abstractmethod
    async def synthesize(text, output_path) -> Path
        # Interface commune

class EdgeTTSProvider(TTSProvider):
    async def synthesize(self, text, output_path):
        # Utilise edge-tts package
        # Voix: fr-FR-DeniseNeural (femme), fr-FR-HenriNeural (homme)
        # Format: MP3 sortie

class CoquiTTSProvider(TTSProvider):
    _model = None  # Singleton

    async def synthesize(self, text, output_path):
        # TTS package local
        # Plus lent mais offline

# Factory function
async def synthesize_speech(text, output_path) -> Path:
    provider = get_tts_provider()  # Based on .env
    return await provider.synthesize(text, output_path)
```

**Configuration (.env)**:
```bash
TTS_PROVIDER=edge-tts               # ou "coqui-tts"
TTS_VOICE=fr-FR-DeniseNeural       # voix Edge TTS
```

**Voix disponibles (Edge TTS)**:
- Fran√ßais: `fr-FR-DeniseNeural` (femme), `fr-FR-HenriNeural` (homme)
- Anglais: `en-US-AriaNeural`, `en-US-GuyNeural`

---

### 3. Agent Layer (`src/agents/jarvis_agent.py`)

**Responsabilit√©**: Logique conversationnelle et g√©n√©ration de r√©ponses

**Architecture**:
```python
class JarvisAgent:
    def __init__(self):
        self.llm = self._initialize_llm()
        self.conversation_history = []  # Garde derniers 10 messages

    def _initialize_llm(self):
        # OpenRouter via OpenAI SDK
        return ChatOpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=os.getenv("OPENROUTER_API_KEY"),
            model=os.getenv("LLM_MODEL", "anthropic/claude-3.5-sonnet"),
            temperature=0.7,
            max_tokens=500
        )

    async def chat(self, user_message: str) -> str:
        # 1. Ajouter message utilisateur √† l'historique
        # 2. Construire prompt avec system + historique
        # 3. Appel LLM
        # 4. Ajouter r√©ponse √† l'historique
        # 5. Retourner r√©ponse

    def clear_history(self):
        # Reset conversation

    def get_history(self) -> list:
        # Retourner historique pour debug/logging

# Singleton
_agent_instance = None
def get_agent() -> JarvisAgent:
    global _agent_instance
    if _agent_instance is None:
        _agent_instance = JarvisAgent()
    return _agent_instance
```

**System Prompt**:
```
Tu es Jarvis, un assistant personnel vocal intelligent et amical.
Tu r√©ponds de mani√®re concise et naturelle pour une synth√®se vocale.
Tu te souviens des conversations pass√©es et tu aides l'utilisateur
dans ses t√¢ches quotidiennes avec professionnalisme et courtoisie.
```

**Configuration (.env)**:
```bash
OPENROUTER_API_KEY=sk-or-v1-xxx
LLM_MODEL=anthropic/claude-3.5-sonnet
```

**Mod√®les support√©s (OpenRouter)**:
- `anthropic/claude-3.5-sonnet` (d√©faut, excellent)
- `meta-llama/llama-3.1-70b-instruct` (gratuit)
- `google/gemini-flash-1.5` (rapide, gratuit)
- `openai/gpt-4o`

---

### 4. Knowledge Graph Layer (`src/graph/graphiti_client.py`)

**√âtat**: Impl√©ment√© mais **non int√©gr√©** au pipeline actuel

**Responsabilit√©**: Gestion du knowledge graph avec Graphiti

**Architecture**:
```python
class GraphitiClient:
    def __init__(self):
        self.driver = GraphDatabase.driver(
            uri=os.getenv("NEO4J_URI"),
            auth=(NEO4J_USER, NEO4J_PASSWORD)
        )
        self.graphiti = Graphiti(
            neo4j_uri=NEO4J_URI,
            neo4j_user=NEO4J_USER,
            neo4j_password=NEO4J_PASSWORD
        )

    async def add_episode(self, text: str, metadata: dict):
        # Ajoute conversation/document au graphe
        # Graphiti extrait automatiquement entit√©s et relations

    async def search(self, query: str, limit: int = 5):
        # Recherche s√©mantique dans le graphe

    async def close(self):
        # Ferme connexion Neo4j

# Singleton
def get_graphiti_client() -> GraphitiClient:
    # Retourne instance unique
```

**Configuration Graphiti** (`config/graphiti_config.yaml`):
```yaml
database:
  uri: ${NEO4J_URI}
  user: ${NEO4J_USER}
  password: ${NEO4J_PASSWORD}

llm_provider:
  provider: openai
  api_key: ${OPENROUTER_API_KEY}
  base_url: https://openrouter.ai/api/v1
  model: ${LLM_MODEL}

embedder:
  model: text-embedding-3-small
  dimensions: 1536

# TODO: Adapter pour domaine assistant personnel
entity_types:
  - Person
  - Event
  - Task
  - Preference
  - Note
  - Contact

relation_types:
  - KNOWS
  - SCHEDULED
  - PREFERS
  - RELATES_TO
```

**Int√©gration √† faire**:
1. Appeler `add_episode()` apr√®s chaque conversation
2. Utiliser `search()` pour enrichir contexte agent
3. D√©finir sch√©ma entit√©s pour assistant personnel

---

### 5. Frontend Layer (`static/`)

#### 5.1 HTML Interface (`index.html`)

**Features**:
- Design moderne avec gradient background
- Bouton microphone push-to-talk (150x150px)
- Canvas pour waveform visualization
- Status indicator (Idle, Listening, Processing, Speaking)
- Zones d'affichage:
  - Transcription utilisateur
  - R√©ponse Jarvis
  - Lecteur audio

**Structure**:
```html
<body>
  <header>
    <h1>ü§ñ Jarvis</h1>
    <div id="status">Idle</div>
  </header>

  <main>
    <canvas id="waveform"></canvas>

    <button id="recordButton" class="mic-button">
      üé§
    </button>

    <div id="transcription"></div>
    <div id="response"></div>
    <audio id="audioPlayer"></audio>
    <div id="error"></div>
  </main>

  <script src="/static/app.js"></script>
</body>
```

#### 5.2 JavaScript Logic (`app.js`)

**Features**:
```javascript
// √âtat global
let mediaRecorder = null;
let audioChunks = [];
let isRecording = false;
let audioContext = null;
let analyser = null;

// Initialisation
async function initAudio() {
  // Demande permission microphone
  // Cr√©e MediaRecorder avec WebM 16kHz mono
  // Configure echo cancellation + noise suppression
}

// Recording
recordButton.addEventListener('mousedown', startRecording);
recordButton.addEventListener('mouseup', stopRecording);
recordButton.addEventListener('touchstart', startRecording);
recordButton.addEventListener('touchend', stopRecording);

async function startRecording() {
  // D√©marre capture audio
  // Lance visualisation waveform
  // Update status: "Listening..."
}

async function stopRecording() {
  // Arr√™te capture
  // Construit blob WebM
  // Upload vers /api/voice/process
  // Update status: "Processing..."
}

// Visualisation
function drawWaveform() {
  // Canvas animation loop
  // Dessine waveform temps r√©el
  requestAnimationFrame(drawWaveform);
}

// Traitement r√©ponse
async function processVoice(audioBlob) {
  const formData = new FormData();
  formData.append('audio', audioBlob, 'recording.webm');

  const response = await fetch('/api/voice/process', {
    method: 'POST',
    body: formData
  });

  const data = await response.json();

  // Afficher transcription et r√©ponse
  // Jouer audio
  audioPlayer.src = data.audio_url;
  audioPlayer.play();
}
```

**Configuration Audio**:
```javascript
const constraints = {
  audio: {
    channelCount: 1,        // Mono
    sampleRate: 16000,      // 16kHz
    echoCancellation: true,
    noiseSuppression: true,
    autoGainControl: true
  }
};
```

---

## Pipeline de Traitement

### Flow Complet (Op√©rationnel)

```
1. USER ACTION
   ‚îî‚îÄ> Maintenir bouton microphone

2. FRONTEND CAPTURE
   ‚îî‚îÄ> MediaRecorder start
   ‚îî‚îÄ> Visualisation waveform
   ‚îî‚îÄ> Status: "Listening..."

3. USER ACTION
   ‚îî‚îÄ> Rel√¢cher bouton

4. FRONTEND PROCESSING
   ‚îî‚îÄ> MediaRecorder stop
   ‚îî‚îÄ> Cr√©er blob WebM
   ‚îî‚îÄ> POST /api/voice/process
   ‚îî‚îÄ> Status: "Processing..."

5. BACKEND: STT PROCESSING
   ‚îî‚îÄ> Sauvegarder fichier temporaire
   ‚îî‚îÄ> Charger provider STT (Whisper)
   ‚îî‚îÄ> Convertir WebM ‚Üí WAV (ffmpeg)
   ‚îî‚îÄ> Transcription ‚Üí texte fran√ßais
   ‚îî‚îÄ> Log: "Transcription: {text}"

6. BACKEND: AGENT PROCESSING
   ‚îî‚îÄ> Charger agent (singleton)
   ‚îî‚îÄ> Ajouter message √† historique
   ‚îî‚îÄ> Construire prompt (system + history)
   ‚îî‚îÄ> Appel OpenRouter/Claude
   ‚îî‚îÄ> Recevoir r√©ponse
   ‚îî‚îÄ> Log: "Agent response: {response}"

7. BACKEND: TTS PROCESSING
   ‚îî‚îÄ> Charger provider TTS (Edge TTS)
   ‚îî‚îÄ> Synth√®se texte ‚Üí MP3
   ‚îî‚îÄ> Sauvegarder dans /static/response_{uuid}.mp3
   ‚îî‚îÄ> Log: "Audio response: {url}"

8. BACKEND: RESPONSE
   ‚îî‚îÄ> Retourner JSON:
       {
         "success": true,
         "transcription": "...",
         "response": "...",
         "audio_url": "/static/response_xxx.mp3"
       }
   ‚îî‚îÄ> Nettoyer fichier input temporaire

9. FRONTEND: DISPLAY
   ‚îî‚îÄ> Afficher transcription
   ‚îî‚îÄ> Afficher r√©ponse texte
   ‚îî‚îÄ> Charger audio player
   ‚îî‚îÄ> Auto-play r√©ponse
   ‚îî‚îÄ> Status: "Speaking..."

10. FRONTEND: COMPLETE
    ‚îî‚îÄ> Audio termin√©
    ‚îî‚îÄ> Status: "Idle"
    ‚îî‚îÄ> Pr√™t pour nouvelle interaction
```

**Temps de traitement typique**:
- Capture audio: 2-5 secondes (dur√©e utilisateur)
- Upload: <1 seconde
- STT (Whisper base): 2-4 secondes
- Agent (Claude): 1-2 secondes
- TTS (Edge): 1-2 secondes
- **Total**: ~5-10 secondes

---

## Flux de Donn√©es

### Formats de Donn√©es

**Audio Input (Frontend ‚Üí Backend)**:
```
Format: WebM Opus
Sample Rate: 16kHz
Channels: Mono (1)
Duration: Variable (user input)
Size: ~10KB/seconde
```

**Transcription (STT ‚Üí Agent)**:
```json
{
  "text": "Bonjour Jarvis, quelle heure est-il ?",
  "language": "fr",
  "confidence": 0.95
}
```

**Agent Response (Agent ‚Üí TTS)**:
```json
{
  "response": "Il est actuellement 14h30. Comment puis-je vous aider ?",
  "model": "anthropic/claude-3.5-sonnet",
  "tokens_used": 45
}
```

**Audio Output (Backend ‚Üí Frontend)**:
```
Format: MP3
Sample Rate: 24kHz
Channels: Mono (1)
Bitrate: 48kbps
URL: /static/response_abc12345.mp3
```

**API Response (Backend ‚Üí Frontend)**:
```json
{
  "success": true,
  "transcription": "Bonjour Jarvis, quelle heure est-il ?",
  "response": "Il est actuellement 14h30...",
  "audio_url": "/static/response_abc12345.mp3",
  "processing_time_ms": 6432
}
```

---

## Impl√©mentation Actuelle

### Modules Complets ‚úÖ

| Module | Fichier | Lignes | Tests | Documentation |
|--------|---------|--------|-------|---------------|
| API FastAPI | `src/api/main.py` | 188 | ‚ùå | ‚úÖ |
| STT Provider | `src/voice/stt.py` | 177 | ‚ùå | ‚úÖ |
| TTS Provider | `src/voice/tts.py` | 161 | ‚ùå | ‚úÖ |
| Agent | `src/agents/jarvis_agent.py` | 144 | ‚ùå | ‚úÖ |
| Graphiti Client | `src/graph/graphiti_client.py` | 170 | ‚ö†Ô∏è | ‚úÖ |
| Web Interface | `static/index.html` | 244 | N/A | ‚úÖ |
| Frontend JS | `static/app.js` | 237 | N/A | ‚úÖ |

**Total code fonctionnel**: ~1321 lignes

---

## Modules √Ä Impl√©menter

### Phase 4: Knowledge Graph Integration

**1. Entity Models** (`src/models/entities.py`)
```python
from pydantic import BaseModel
from datetime import datetime
from typing import Optional, List

class Person(BaseModel):
    name: str
    relationship: Optional[str]
    notes: Optional[str]

class Event(BaseModel):
    title: str
    date: datetime
    location: Optional[str]
    participants: List[str]

class Task(BaseModel):
    description: str
    due_date: Optional[datetime]
    priority: str  # low, medium, high
    completed: bool = False

class Preference(BaseModel):
    category: str  # food, music, etc.
    item: str
    strength: float  # 0-1

class Note(BaseModel):
    content: str
    tags: List[str]
    created_at: datetime
```

**2. Entity Extraction** (`src/rag/entity_extractor.py`)
```python
class EntityExtractor:
    def __init__(self, llm):
        self.llm = llm

    async def extract_entities(self, conversation: str) -> dict:
        # Utilise LLM pour extraire entit√©s structur√©es
        # Retourne dict avec Person, Event, Task, etc.

    async def extract_preferences(self, text: str) -> List[Preference]:
        # D√©tecte pr√©f√©rences utilisateur

    async def extract_tasks(self, text: str) -> List[Task]:
        # D√©tecte t√¢ches √† faire
```

**3. GraphRAG Implementation** (`src/rag/graphrag.py`)
```python
class GraphRAG:
    def __init__(self, graphiti_client, llm):
        self.graph = graphiti_client
        self.llm = llm

    async def enrich_context(self, user_query: str) -> str:
        # 1. Recherche s√©mantique dans graphe
        results = await self.graph.search(user_query, limit=5)

        # 2. Construire contexte enrichi
        context = self._build_context(results)

        # 3. Retourner pour injection dans agent prompt
        return context

    async def update_graph(self, conversation: str, entities: dict):
        # Mise √† jour graphe avec nouvelles infos
        await self.graph.add_episode(conversation, entities)
```

**4. Integration dans Pipeline** (`src/api/main.py`)
```python
@app.post("/api/voice/process")
async def process_voice(audio: UploadFile):
    # ... STT ...

    # GraphRAG enrichment
    graphrag = get_graphrag()
    context = await graphrag.enrich_context(transcription)

    # Agent avec contexte enrichi
    agent = get_agent()
    response = await agent.chat(transcription, context=context)

    # ... TTS ...

    # Update knowledge graph
    entities = await extract_entities(transcription, response)
    await graphrag.update_graph(
        f"User: {transcription}\nJarvis: {response}",
        entities
    )

    # ... return ...
```

---

### Phase 5: ESP32 Hardware

**Firmware Architecture** (`esp32/src/main.cpp`)
```cpp
// 1. Wake Word Detection
void detectWakeWord() {
  // Porcupine ou Edge Impulse
  // D√©tecte "Hey Jarvis"
}

// 2. Audio Capture
void captureAudio() {
  // I2S microphone INMP441
  // Buffer audio 16kHz mono
}

// 3. WiFi Upload
void uploadAudio(uint8_t* buffer, size_t length) {
  // HTTP POST vers /api/voice/process
  // Multipart/form-data
}

// 4. Download & Play Response
void playResponse(String audioUrl) {
  // HTTP GET audio MP3
  // I2S speaker MAX98357A
}

// Main loop
void loop() {
  if (detectWakeWord()) {
    ledOn();
    captureAudio();
    uploadAudio();
    playResponse();
    ledOff();
  }
}
```

---

## D√©cisions Architecturales

### 1. Pourquoi Singleton Pattern ?

**Raison**: √âviter de charger mod√®les lourds (Whisper, TTS) √† chaque requ√™te

**Impact**: Premi√®re requ√™te lente (~10s), suivantes rapides (~5s)

**Alternative**: Pool de workers pr√©-charg√©s (plus complexe)

---

### 2. Pourquoi OpenRouter ?

**Raison**: Acc√®s √† 100+ mod√®les via une seule API

**Avantages**:
- Switch entre Claude, GPT-4, Llama sans changer code
- Meilleurs prix (routing automatique)
- Pas de vendor lock-in

**Alternative**: Anthropic API direct, OpenAI direct (moins flexible)

---

### 3. Pourquoi FastAPI ?

**Raison**: Framework async moderne, documentation auto, performances

**Avantages**:
- Async/await natif (important pour I/O)
- Auto-g√©n√©ration OpenAPI docs
- Type hints Pydantic
- WebSocket support pour futur streaming

**Alternative**: Flask (sync, moins performant), Django (trop lourd)

---

### 4. Pourquoi Neo4j + Graphiti ?

**Raison**: Knowledge graph dynamique avec extraction auto

**Avantages Neo4j**:
- Graph database mature et performante
- Cypher query language puissant
- Visualisation graphe int√©gr√©e

**Avantages Graphiti**:
- Extraction entit√©s automatique via LLM
- Gestion relations temporelles
- API simple

**Alternative**:
- PostgreSQL + pgvector (moins adapt√© aux relations)
- Custom graph implementation (r√©inventer la roue)

---

### 5. Pourquoi Whisper Local vs Cloud STT ?

**Raison**: Privacy + co√ªt z√©ro

**Trade-offs**:
| | Whisper Local | Google STT | Groq |
|-|---------------|------------|------|
| **Co√ªt** | Gratuit | $0.006/15s | Gratuit |
| **Privacy** | ‚úÖ Total | ‚ùå Cloud | ‚ùå Cloud |
| **Vitesse** | ‚ö†Ô∏è Lent (CPU) | ‚úÖ Rapide | ‚úÖ Tr√®s rapide |
| **Qualit√©** | ‚úÖ Excellent | ‚úÖ Excellent | ‚úÖ Excellent |
| **Offline** | ‚úÖ Oui | ‚ùå Non | ‚ùå Non |

**D√©cision**: Local par d√©faut, Groq en option

---

### 6. Pourquoi Docker ?

**Raison**: Environnement reproductible, d√©ploiement simplifi√©

**Avantages**:
- M√™me environnement dev/prod
- Isolation d√©pendances syst√®me (ffmpeg, etc.)
- Neo4j containeris√©
- Scaling facile (futur: docker swarm, k8s)

**Alternative**:
- Virtualenv (probl√®mes d√©pendances syst√®me)
- Installation manuelle (non reproductible)

---

## Prochaines √âtapes Techniques

### Priorit√© 1: GraphRAG Integration (Phase 4)

1. Cr√©er `src/models/entities.py` avec Pydantic models
2. Impl√©menter `src/rag/entity_extractor.py`
3. Int√©grer extraction dans `/api/voice/process`
4. Tester mise √† jour automatique graphe
5. Impl√©menter recherche s√©mantique pour enrichir contexte agent

**Estimation**: 2-3 jours de d√©veloppement

---

### Priorit√© 2: Tests (Phase 4)

1. Tests unitaires pour chaque provider (STT, TTS, Agent)
2. Tests d'int√©gration pipeline complet
3. Tests Graphiti (connexion, add_episode, search)
4. Mock LLM pour tests rapides
5. CI/CD GitHub Actions

**Estimation**: 1-2 jours

---

### Priorit√© 3: ESP32 (Phase 5)

*Attend r√©ception mat√©riel*

1. Setup PlatformIO
2. Test microphone I2S
3. Test speaker I2S
4. Impl√©mentation wake word (Porcupine)
5. Communication WiFi avec backend
6. Optimisation latence

**Estimation**: 1 semaine apr√®s r√©ception mat√©riel

---

**Document maintenu √† jour au fur et √† mesure du d√©veloppement**
