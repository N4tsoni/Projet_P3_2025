# Knowledge Graph Pipeline Architecture

## ğŸ“‹ Vue d'ensemble

Le systÃ¨me de pipeline modulaire pour la construction de Knowledge Graphs (KG) est conÃ§u pour traiter des documents de diffÃ©rents formats et extraire des entitÃ©s et relations structurÃ©es.

## ğŸ—ï¸ Structure

```
backend/src/kg/
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py              # Classes de base (Stage, StageResult)
â”‚   â”œâ”€â”€ pipeline.py          # Orchestrateur principal (Pipeline, PipelineContext)
â”‚   â”œâ”€â”€ factory.py           # Factory pour pipelines prÃ©configurÃ©es
â”‚   â””â”€â”€ stages/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ parsing.py       # Stage 1: Parsing de documents
â”‚       â”œâ”€â”€ chunking.py      # Stage 2: DÃ©coupage en chunks
â”‚       â”œâ”€â”€ embedding.py     # Stage 3: GÃ©nÃ©ration d'embeddings
â”‚       â”œâ”€â”€ ner.py           # Stage 4: Named Entity Recognition
â”‚       â”œâ”€â”€ extraction.py    # Stage 5: Extraction entitÃ©s/relations (LLM)
â”‚       â”œâ”€â”€ transformation.py # Stage 6: Transformation des donnÃ©es
â”‚       â”œâ”€â”€ enrichment.py    # Stage 7: Enrichissement
â”‚       â”œâ”€â”€ validation.py    # Stage 8: Validation
â”‚       â””â”€â”€ storage.py       # Stage 9: Stockage Neo4j
â””â”€â”€ pipeline_example.py      # Exemples d'utilisation
```

## ğŸ”„ Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Document   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. ParsingStage    â”‚  Parse CSV, JSON, PDF, TXT
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. ChunkingStage   â”‚  DÃ©coupe en chunks (optionnel)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. EmbeddingStage  â”‚  GÃ©nÃ¨re embeddings (optionnel)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. NERStage        â”‚  Named Entity Recognition (optionnel)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. ExtractionStage â”‚  Extraction LLM (Claude)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. TransformationStage â”‚  Normalisation des donnÃ©es
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. EnrichmentStage â”‚  Enrichissement contextuel
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  8. ValidationStage â”‚  Validation qualitÃ©
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  9. StorageStage    â”‚  Stockage Neo4j
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Done  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Les 9 Stages

### 1. **ParsingStage** - Parsing de Documents
**ResponsabilitÃ©**: Convertir les documents bruts en donnÃ©es structurÃ©es

**Formats supportÃ©s**:
- âœ… CSV (implÃ©mentÃ©)
- â³ JSON (Ã  implÃ©menter)
- â³ PDF (Ã  implÃ©menter)
- â³ TXT (Ã  implÃ©menter)
- â³ XLSX (Ã  implÃ©menter)

**EntrÃ©e**: `context.file_path`, `context.file_format`
**Sortie**: `context.raw_data`, `context.metadata`

### 2. **ChunkingStage** - DÃ©coupage en Chunks
**ResponsabilitÃ©**: DÃ©couper les textes longs en chunks gÃ©rables

**Configuration**:
- `chunk_size`: Taille maximale d'un chunk (dÃ©faut: 1000 caractÃ¨res)
- `chunk_overlap`: Chevauchement entre chunks (dÃ©faut: 200 caractÃ¨res)

**EntrÃ©e**: `context.raw_data`
**Sortie**: `context.chunks`

**Note**: Pour les CSV, chaque ligne devient un chunk. Pour les textes, on dÃ©coupe avec chevauchement.

### 3. **EmbeddingStage** - GÃ©nÃ©ration d'Embeddings
**ResponsabilitÃ©**: GÃ©nÃ©rer des reprÃ©sentations vectorielles des chunks

**ModÃ¨les supportÃ©s**:
- Sentence-Transformers (all-MiniLM-L6-v2, etc.)
- OpenAI embeddings
- Custom models

**EntrÃ©e**: `context.chunks`
**Sortie**: `context.embeddings`

**Usage**: Pour recherche sÃ©mantique, similaritÃ©, clustering

### 4. **NERStage** - Named Entity Recognition
**ResponsabilitÃ©**: Identifier les entitÃ©s nommÃ©es avec des modÃ¨les NLP

**ModÃ¨les supportÃ©s**:
- spaCy (en_core_web_sm, fr_core_news_sm, etc.)
- Transformers (BERT-NER, etc.)

**EntrÃ©e**: `context.chunks`
**Sortie**: Enrichit `context.entities`

**EntitÃ©s dÃ©tectÃ©es**: PERSON, ORG, GPE, DATE, etc.

### 5. **ExtractionStage** - Extraction LLM
**ResponsabilitÃ©**: Extraire entitÃ©s et relations avec Claude (LLM)

**Configuration**:
- `batch_size`: Nombre de records par batch (dÃ©faut: 50)

**Agents**:
- `EntityExtractorAgent`: Extrait entitÃ©s structurÃ©es
- `RelationExtractorAgent`: Extrait relations entre entitÃ©s

**EntrÃ©e**: `context.raw_data`, `context.metadata`
**Sortie**: `context.entities`, `context.relations`

**Note**: C'est le stage principal d'extraction intelligent.

### 6. **TransformationStage** - Transformation
**ResponsabilitÃ©**: Normaliser et transformer les donnÃ©es extraites

**Transformations**:
- Normalisation des noms (lowercase, title case)
- Conversion de types (dates, nombres)
- DÃ©duplication avancÃ©e
- Merge de propriÃ©tÃ©s

**EntrÃ©e**: `context.entities`, `context.relations`
**Sortie**: Transforme en place

### 7. **EnrichmentStage** - Enrichissement
**ResponsabilitÃ©**: Enrichir les donnÃ©es avec des informations externes

**Sources d'enrichissement**:
- Wikipedia / DBpedia
- Wikidata
- APIs externes
- Calcul de scores (centralitÃ©, importance)
- Ajout de mÃ©tadonnÃ©es temporelles

**EntrÃ©e**: `context.entities`, `context.relations`
**Sortie**: `context.enriched_entities`, `context.enriched_relations`

### 8. **ValidationStage** - Validation
**ResponsabilitÃ©**: Valider la qualitÃ© et cohÃ©rence des donnÃ©es

**Validations**:
- Champs requis prÃ©sents
- Types de donnÃ©es corrects
- RÃ©fÃ©rences valides (entitÃ©s existent)
- Pas de doublons
- Contraintes mÃ©tier respectÃ©es

**Configuration**:
- `strict`: Si True, Ã©choue sur erreur. Si False, continue avec warnings

**EntrÃ©e**: `context.enriched_entities`, `context.enriched_relations`
**Sortie**: `context.validation_results`

### 9. **StorageStage** - Stockage Neo4j
**ResponsabilitÃ©**: Persister le graphe dans Neo4j

**OpÃ©rations**:
- CrÃ©ation de nÅ“uds (MERGE sur name)
- CrÃ©ation de relations (MERGE)
- Transactions batch pour performance
- Gestion d'erreurs

**EntrÃ©e**: `context.enriched_entities`, `context.enriched_relations`
**Sortie**: `context.storage_ids`

## ğŸš€ Utilisation

### MÃ©thode 1: Factory (RecommandÃ©)

```python
from kg.pipeline.factory import PipelineFactory

# Pipeline pour CSV
pipeline = PipelineFactory.create_csv_pipeline()

# Pipeline pour textes (PDF, TXT)
pipeline = PipelineFactory.create_text_pipeline()

# Pipeline minimal (rapide)
pipeline = PipelineFactory.create_minimal_pipeline()

# Pipeline personnalisÃ©e
pipeline = PipelineFactory.create_custom_pipeline(
    include_chunking=False,
    include_ner=False,
    batch_size=100
)
```

### MÃ©thode 2: Construction Manuelle

```python
from kg.pipeline import Pipeline
from kg.pipeline.stages import ParsingStage, ExtractionStage, StorageStage

pipeline = Pipeline(name="My Pipeline")
pipeline.add_stage(ParsingStage())
pipeline.add_stage(ExtractionStage(batch_size=50))
pipeline.add_stage(StorageStage())
```

### ExÃ©cution

```python
from pathlib import Path

context = await pipeline.execute(
    file_path=Path("data/movies.csv"),
    filename="movies.csv",
    file_format="csv"
)

# RÃ©sultats
print(f"Success: {context.is_successful()}")
print(f"Entities: {len(context.entities)}")
print(f"Relations: {len(context.relations)}")
print(f"Duration: {context.get_duration():.2f}s")

# RÃ©sultats par stage
for result in context.stage_results:
    print(f"{result.stage_name}: {result.status} ({result.duration_seconds:.2f}s)")
```

## ğŸ›ï¸ Configuration des Pipelines

### Pipeline CSV (StructurÃ©)

Pour donnÃ©es tabulaires structurÃ©es:

```
ParsingStage â†’ ExtractionStage â†’ TransformationStage â†’ ValidationStage â†’ StorageStage
```

**Stages exclus**: Chunking, Embedding, NER (pas nÃ©cessaires pour CSV)

### Pipeline Text (Non-structurÃ©)

Pour documents texte (PDF, TXT, etc.):

```
ParsingStage â†’ ChunkingStage â†’ EmbeddingStage â†’ NERStage â†’
ExtractionStage â†’ TransformationStage â†’ EnrichmentStage â†’
ValidationStage â†’ StorageStage
```

**Stages inclus**: Tous les stages pour traitement complet

### Pipeline Minimal (Rapide)

Pour tests ou cas simples:

```
ParsingStage â†’ ExtractionStage â†’ StorageStage
```

**DurÃ©e**: ~30% plus rapide que pipeline complet

## ğŸ”§ Personnalisation

### DÃ©sactiver un Stage

```python
pipeline = PipelineFactory.create_default_pipeline()

# DÃ©sactiver validation
validation_stage = pipeline.get_stage("ValidationStage")
validation_stage.disable()

# Le stage sera skippÃ© lors de l'exÃ©cution
```

### Ajouter/Retirer des Stages

```python
# Retirer un stage
pipeline.remove_stage("EmbeddingStage")

# Ajouter un stage personnalisÃ©
from kg.pipeline.base import Stage

class MyCustomStage(Stage):
    async def execute(self, context):
        # ... votre logique
        return StageResult(...)

pipeline.add_stage(MyCustomStage())
```

### Modifier les ParamÃ¨tres

```python
pipeline = Pipeline()
pipeline.add_stage(ChunkingStage(chunk_size=2000, chunk_overlap=400))
pipeline.add_stage(ExtractionStage(batch_size=100))
pipeline.add_stage(ValidationStage(strict=True))
```

## ğŸ“Š PipelineContext

Le `PipelineContext` est l'objet partagÃ© entre tous les stages.

**DonnÃ©es d'entrÃ©e**:
- `file_path`: Chemin du fichier
- `filename`: Nom du fichier
- `file_format`: Format (csv, json, pdf, txt)
- `document`: Objet Document pour tracking

**DonnÃ©es intermÃ©diaires** (remplies par stages):
- `raw_data`: DonnÃ©es parsÃ©es brutes
- `metadata`: MÃ©tadonnÃ©es du fichier
- `chunks`: Chunks de texte
- `embeddings`: Embeddings vectoriels
- `entities`: EntitÃ©s extraites
- `relations`: Relations extraites
- `enriched_entities`: EntitÃ©s enrichies
- `enriched_relations`: Relations enrichies
- `validation_results`: RÃ©sultats de validation
- `storage_ids`: IDs Neo4j

**Tracking**:
- `start_time`: DÃ©but du pipeline
- `stage_results`: RÃ©sultats de chaque stage
- `errors`: Liste des erreurs

**MÃ©thodes**:
- `get_duration()`: DurÃ©e totale
- `is_successful()`: SuccÃ¨s ou Ã©chec
- `get_stage_result(name)`: RÃ©sultat d'un stage spÃ©cifique
- `to_dict()`: Convertir en dictionnaire

## ğŸ§ª Tests

```bash
# Lancer les exemples
cd backend
python -m kg.pipeline_example

# Tester avec un fichier
python -m kg.pipeline_example data/test_datasets/movies_sample.csv
```

## ğŸ“ TODO

### Stages Ã  complÃ©ter

- [ ] **ParsingStage**: Ajouter parsers JSON, PDF, TXT, XLSX
- [ ] **ChunkingStage**: ImplÃ©menter chunking avec overlap pour texte
- [ ] **EmbeddingStage**: IntÃ©grer sentence-transformers
- [ ] **NERStage**: IntÃ©grer spaCy et transformers
- [ ] **TransformationStage**: ImplÃ©menter normalisation avancÃ©e
- [ ] **EnrichmentStage**: Ajouter sources externes (Wikipedia, DBpedia)
- [ ] **ValidationStage**: ImplÃ©menter rÃ¨gles de validation complÃ¨tes

### FonctionnalitÃ©s Ã  ajouter

- [ ] Support streaming pour gros fichiers
- [ ] ParallÃ©lisation des stages indÃ©pendants
- [ ] Cache des rÃ©sultats intermÃ©diaires
- [ ] Reprise sur Ã©chec (checkpointing)
- [ ] MÃ©triques et monitoring (Prometheus)
- [ ] Pipeline DAG (non-linÃ©aire)
- [ ] UI pour visualiser pipeline execution

## ğŸ“ Bonnes Pratiques

1. **Utilisez les factories** pour les cas courants
2. **DÃ©sactivez les stages inutiles** pour amÃ©liorer performance
3. **Testez avec pipeline minimal** avant d'activer tous les stages
4. **Ajustez batch_size** selon la taille des documents
5. **Validez en mode non-strict** pendant dÃ©veloppement
6. **Surveillez les durÃ©es** de chaque stage pour optimisation

## ğŸ“š RÃ©fÃ©rences

- [FastAPI Async](https://fastapi.tiangolo.com/async/)
- [Neo4j Python Driver](https://neo4j.com/docs/python-manual/current/)
- [Sentence Transformers](https://www.sbert.net/)
- [spaCy NER](https://spacy.io/usage/linguistic-features#named-entities)
- [LangChain Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)

---

**Version**: 1.0.0
**Date**: 2025-01-07
**Auteur**: Jarvis Backend Team
